{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "dqn Flappy Bird.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "WFKS109dRrkE",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "outputId": "33f310b1-45d8-4170-89e9-58a84763b70a"
   },
   "source": [
    "!pip install pygame\n",
    "!git clone https://github.com/elitcenk/flappy-bird-agent.git\n",
    "  \n",
    "import os\n",
    "os.chdir('flappy-bird-agent')"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
      "\u001b[K     |████████████████████████████████| 11.4MB 2.4MB/s \n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-1.9.6\n",
      "Cloning into 'flappy-bird-agent'...\n",
      "remote: Enumerating objects: 41, done.\u001b[K\n",
      "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
      "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
      "remote: Total 41 (delta 16), reused 37 (delta 15), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (41/41), done.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBDfgjXxSPWu",
    "colab_type": "text"
   },
   "source": [
    "Colab don't support monitor. For this we use headless."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XBgPOfxESNsX",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "os.putenv('SDL_VIDEODRIVER', 'fbcon')\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "print(\"Headless done\") "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRaktP55SiCX",
    "colab_type": "text"
   },
   "source": [
    "FlappyBird game path added."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KOlvYRpjSiit",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"game/\")\n",
    "\n",
    "print(\"Game added.\") "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20otRT1zSw41",
    "colab_type": "text"
   },
   "source": [
    "Configuration of DQN of Flappy Bird."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SdpcdJE5SxHF",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "ACTIONS = 2  # number of valid actions\n",
    "GAMMA = 0.99  # decay rate of past observations\n",
    "OBSERVATION = 100.  # timesteps to observe before training\n",
    "N_EP = 10000   # number of episode\n",
    "N_SAVE = 500  # every N_SAVE number save the model\n",
    "EXPLORE = 10000.  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001  # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1  # starting value of epsilon\n",
    "REPLAY_MEMORY = 5000  # number of previous transitions to remember\n",
    "BATCH = 32  # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "IMG_ROWS, IMG_COLS = 80, 80\n",
    "IMG_CHANNELS = 4  # Stacked 4 frames\n",
    "print(\"imported successfully\") "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biIAF_LUS5xr",
    "colab_type": "text"
   },
   "source": [
    "Import some library"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UYOsUaftU_qm",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "outputId": "876b74b2-38f7-4809-a695-c3200baf7e98"
   },
   "source": [
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "\n",
    "import game.wrapped_flappy_bird as game\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRZEPhLiU3VG",
    "colab_type": "text"
   },
   "source": [
    "Added DQN agent of Flappy Bird"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C5UIvWcbS54z",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class DQNAgent():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.game_state = game.GameState()\n",
    "        self.memory = deque(maxlen=REPLAY_MEMORY)\n",
    "        self.scores = deque(maxlen=100)\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        print(\"Now we build the model\")\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(32, (8, 8), input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS), strides=(4, 4), padding=\"same\"))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Conv2D(64, (4, 4), strides=(2, 2), padding=\"same\"))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(512))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(ACTIONS))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))\n",
    "        print(\"We finish building the model\")\n",
    "\n",
    "    def train(self):\n",
    "        # We go to training mode\n",
    "        self.OBSERVE = OBSERVATION\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.run_model()\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.OBSERVE = 999999999  # We keep observe, never train\n",
    "        self.epsilon = FINAL_EPSILON\n",
    "        self.restore_model()\n",
    "        self.run_model()\n",
    "\n",
    "    def restore_model(self):\n",
    "        weight_path = F\"/content/gdrive/My Drive/model.h5\"\n",
    "        print(\"Now we load weight\")\n",
    "        self.model.load_weights(weight_path)\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))\n",
    "        print(\"Weight load successfully\")\n",
    "\n",
    "    def run_model(self):\n",
    "        for step in range(N_EP):\n",
    "            score = 0\n",
    "            current_stack = self.create_first_stack()\n",
    "            while True:\n",
    "                action, action_index = self.act(current_stack, step)\n",
    "                # run the selected action and observed next state and reward\n",
    "                frame, reward, terminal = self.game_state.frame_step(action)\n",
    "                frame = self.preprocess(frame)\n",
    "                frame = frame.reshape(1, frame.shape[0], frame.shape[1], 1)  # 1x[IMG_COLS]x[IMG_ROWS]x1\n",
    "                self.remember(action_index, current_stack, reward, terminal, frame)\n",
    "                current_stack = np.append(frame, current_stack[:, :, :, :3], axis=3)\n",
    "                Q_sa, loss = self.replay(step)\n",
    "                score = score + reward\n",
    "                if terminal:\n",
    "                    break\n",
    "            self.scores.append(score)\n",
    "            print(\"Episode {} score: {}\".format(step + 1, score))\n",
    "            self.mean_score = np.mean(self.scores)\n",
    "            self.print(step, score)\n",
    "\n",
    "    def print(self, step, score):\n",
    "        if (step + 1) % 5 == 0:\n",
    "            print(\"Episode {}, score: {}, exploration at {}%, mean of last 100 episodes was {}\".format(step + 1, score, self.epsilon, self.mean_score))\n",
    "\n",
    "        if (step + 1) % N_SAVE == 0 and step > 0:\n",
    "            self.save_model()\n",
    "\n",
    "    def save_model(self):\n",
    "        print(\"Now we save model\")\n",
    "        json_path = F\"/content/gdrive/My Drive/model.json\"\n",
    "        weight_path = F\"/content/gdrive/My Drive/model.h5\"\n",
    "        self.model.save_weights(weight_path, overwrite=True)\n",
    "        with open(json_path, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "\n",
    "    def remember(self, action_index, current_stack, reward, terminal, frame):\n",
    "        # store the transition in memory\n",
    "        self.memory.append((current_stack, action_index, reward, frame, terminal))\n",
    "\n",
    "    def replay(self, step):\n",
    "        Q_sa = 0\n",
    "        loss = 0\n",
    "        # only train if done observing\n",
    "        if step > self.OBSERVE:\n",
    "            # sample a minibatch to train on\n",
    "            minibatch = random.sample(self.memory, BATCH)\n",
    "            # Now we do the experience replay\n",
    "            current_stack, action_index, reward, frame, terminal = zip(*minibatch)\n",
    "            current_stack = np.concatenate(current_stack)\n",
    "            frame = np.concatenate(frame)\n",
    "            next_stack = np.append(frame, current_stack[:, :, :, :3], axis=3)\n",
    "            targets = self.model.predict(current_stack)\n",
    "            Q_sa = self.model.predict(next_stack)\n",
    "            targets[range(BATCH), action_index] = reward + GAMMA * np.max(Q_sa, axis=1) * np.invert(terminal)\n",
    "            loss += self.model.train_on_batch(current_stack, targets)\n",
    "        return Q_sa, loss\n",
    "\n",
    "    def act(self, frame_stack, step):\n",
    "        action_index = 0\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        # choose an action epsilon greedy\n",
    "        if step % FRAME_PER_ACTION == 0:\n",
    "            if random.random() <= self.epsilon:\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else:\n",
    "                q = self.model.predict(frame_stack)  # input a stack of [IMG_CHANNELS] images, get the prediction\n",
    "                max_Q = np.argmax(q)\n",
    "                action_index = max_Q\n",
    "                a_t[max_Q] = 1\n",
    "\n",
    "        # We reduced the epsilon gradually\n",
    "        if self.epsilon > FINAL_EPSILON and step > self.OBSERVE:\n",
    "            self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "        return a_t, action_index\n",
    "\n",
    "    def preprocess(self, frame):\n",
    "        frame = skimage.color.rgb2gray(frame)\n",
    "        frame = skimage.transform.resize(frame, (IMG_ROWS, IMG_COLS))\n",
    "        frame = skimage.exposure.rescale_intensity(frame, out_range=(0, 255))\n",
    "        return frame / 255.0\n",
    "\n",
    "    def create_first_stack(self):\n",
    "        # get the first state by doing nothing and preprocess the image\n",
    "        do_nothing = np.zeros(ACTIONS)\n",
    "        do_nothing[0] = 1\n",
    "        frame, reward, terminal = self.game_state.frame_step(do_nothing)\n",
    "        frame = self.preprocess(frame)\n",
    "        frame_stack = np.stack((frame, frame, frame, frame), axis=2)\n",
    "        # In Keras, need to reshape\n",
    "        frame_stack = frame_stack.reshape(1, frame_stack.shape[0], frame_stack.shape[1], frame_stack.shape[2])  # 1*[IMG_COLS]x[IMG_ROWS]*[IMG_CHANNELS]\n",
    "        return frame_stack\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PREyzJqZV4_6",
    "colab_type": "text"
   },
   "source": [
    "Mount the Google Drive for save and load model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7tGa_rx2V5L3",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "outputId": "351bcb83-7d2a-4a33-df7c-f447ed5e0107"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "\n",
    "print(\"mounded GDrive\")  "
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive/\n",
      "mounded GDrive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DL9OOwQVZ8_",
    "colab_type": "text"
   },
   "source": [
    "Create agent and train. If yo want to evaluate you call evaluate method."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o2z9LllBVaFe",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "06b3c37f-089e-46b4-958f-838e949cda46"
   },
   "source": [
    "agent = DQNAgent()\n",
    "agent.train()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "We finish building the model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/skimage/exposure/exposure.py:351: RuntimeWarning: invalid value encountered in true_divide\n",
      "  image = (image - imin) / float(imax - imin)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Episode 1 score: -0.6699999999999999\n",
      "Episode 2 score: -0.6799999999999999\n",
      "Episode 3 score: -0.4299999999999997\n",
      "Episode 4 score: -0.83\n",
      "Episode 5 score: -0.83\n",
      "Episode 5, score: -0.83, exploration at 0.1%, mean of last 100 episodes was -0.688\n",
      "Episode 6 score: -0.83\n",
      "Episode 7 score: -0.6399999999999999\n",
      "Episode 8 score: -0.83\n",
      "Episode 9 score: 0.6300000000000003\n",
      "Episode 10 score: -0.83\n",
      "Episode 10, score: -0.83, exploration at 0.1%, mean of last 100 episodes was -0.594\n",
      "Episode 11 score: -0.83\n",
      "Episode 12 score: -0.5199999999999998\n",
      "Episode 13 score: -0.6599999999999999\n",
      "Episode 14 score: 0.9200000000000006\n",
      "Episode 15 score: -0.6399999999999999\n",
      "Episode 15, score: -0.6399999999999999, exploration at 0.1%, mean of last 100 episodes was -0.5113333333333332\n",
      "Episode 16 score: -0.83\n",
      "Episode 17 score: -0.5199999999999998\n",
      "Episode 18 score: -0.83\n",
      "Episode 19 score: -0.83\n",
      "Episode 20 score: -0.83\n",
      "Episode 20, score: -0.83, exploration at 0.1%, mean of last 100 episodes was -0.5754999999999998\n",
      "Episode 21 score: -0.5199999999999998\n",
      "Episode 22 score: -0.6399999999999999\n",
      "Episode 23 score: -0.83\n",
      "Episode 24 score: -0.5199999999999998\n",
      "Episode 25 score: -0.83\n",
      "Episode 25, score: -0.83, exploration at 0.1%, mean of last 100 episodes was -0.5939999999999999\n",
      "Episode 26 score: -0.4299999999999997\n",
      "Episode 27 score: -0.6399999999999999\n",
      "Episode 28 score: 0.7900000000000005\n",
      "Episode 29 score: -0.5199999999999998\n",
      "Episode 30 score: -0.83\n",
      "Episode 30, score: -0.83, exploration at 0.1%, mean of last 100 episodes was -0.5493333333333331\n",
      "Episode 31 score: 1.9899999999999993\n",
      "Episode 32 score: -0.83\n",
      "Episode 33 score: 0.9300000000000006\n",
      "Episode 34 score: 0.7200000000000004\n",
      "Episode 35 score: -0.6699999999999999\n",
      "Episode 35, score: -0.6699999999999999, exploration at 0.1%, mean of last 100 episodes was -0.4097142857142856\n",
      "Episode 36 score: -0.48999999999999977\n",
      "Episode 37 score: 0.8300000000000005\n",
      "Episode 38 score: -0.83\n",
      "Episode 39 score: -0.83\n",
      "Episode 40 score: 0.9300000000000006\n",
      "Episode 40, score: 0.9300000000000006, exploration at 0.1%, mean of last 100 episodes was -0.36824999999999986\n",
      "Episode 41 score: 0.7900000000000005\n",
      "Episode 42 score: -0.83\n",
      "Episode 43 score: -0.83\n",
      "Episode 44 score: -0.4199999999999997\n",
      "Episode 45 score: -0.83\n",
      "Episode 45, score: -0.83, exploration at 0.1%, mean of last 100 episodes was -0.37444444444444425\n",
      "Episode 46 score: -0.6499999999999999\n",
      "Episode 47 score: -0.83\n",
      "Episode 48 score: 0.8100000000000005\n",
      "Episode 49 score: -0.83\n",
      "Episode 50 score: -0.5199999999999998\n",
      "Episode 50, score: -0.5199999999999998, exploration at 0.1%, mean of last 100 episodes was -0.3773999999999998\n",
      "Episode 51 score: -0.83\n",
      "Episode 52 score: -0.83\n",
      "Episode 53 score: -0.83\n",
      "Episode 54 score: -0.83\n",
      "Episode 55 score: 0.8300000000000005\n",
      "Episode 55, score: 0.8300000000000005, exploration at 0.1%, mean of last 100 episodes was -0.388363636363636\n",
      "Episode 56 score: -0.83\n",
      "Episode 57 score: -0.83\n",
      "Episode 58 score: -0.83\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}